{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![intro](https://images.pexels.com/photos/1660990/pexels-photo-1660990.jpeg)","metadata":{"execution":{"iopub.status.busy":"2022-10-02T16:39:05.629776Z","iopub.execute_input":"2022-10-02T16:39:05.630159Z","iopub.status.idle":"2022-10-02T16:39:06.629224Z","shell.execute_reply.started":"2022-10-02T16:39:05.630127Z","shell.execute_reply":"2022-10-02T16:39:06.627994Z"}}},{"cell_type":"markdown","source":"# VQGAN in brief:\n**VQGAN** stands for **Vector Quantized Generative Adversarial Network**, while **CLIP** stands for **Contrastive Image-Language Pretraining**. Whenever we say VQGAN-CLIP1, we refer to the interaction between these two networks. They’re separate models that work in tandem. The way they work is that VQGAN generates the images, while CLIP judges how well an image matches our text prompt. This interaction guides our generator to produce more accurate images.\n___","metadata":{"execution":{"iopub.status.busy":"2022-10-02T16:39:13.711233Z","iopub.execute_input":"2022-10-02T16:39:13.711697Z","iopub.status.idle":"2022-10-02T16:39:13.722278Z","shell.execute_reply.started":"2022-10-02T16:39:13.711656Z","shell.execute_reply":"2022-10-02T16:39:13.720836Z"}}},{"cell_type":"markdown","source":"**VQGAN** employs  two-stage structure by learning an intermediary representation before feeding it to a transformer. However, instead of downsampling the image, VQGAN uses a codebook to represent visual parts. The authors did not model the image from a pixel-level directly, but instead from the codewords of the learned codebook.\n\n![vqgan](https://compvis.github.io/taming-transformers/paper/teaser.png)\n\n\nVQGAN was able to solve Transformer’s scaling problem by using an intermediate representation known as a codebook. This codebook serves as the bridge for the two-stage approach found in most image transformer techniques. The VQGAN learns a codebook of context-rich visual parts, whose composition is then modeled with an autoregressive transformer.\n\nThe codebook is generated through a process called vector quantization (VQ), i.e., the “VQ” part of “VQGAN.” Vector quantization is a signal processing technique for encoding vectors. It represents all visual parts found in the convolutional step in a quantized form, making it less computationally expensive once passed to a transformer network.\n\nOne can think of vector quantization as a process of dividing vectors into groups that have approximately the same number of points closest to them. Each group is then represented by a centroid (codeword), usually obtained via k-means or any other clustering algorithm. In the end, one learns a dictionary of centroids (codebook) and their corresponding members.\n___","metadata":{}},{"cell_type":"markdown","source":"# Brief about CLIP:\n    \n**CLIP( Contrastive Language–Image Pre-training )** a model trained to determine which caption from a set of captions best fits with a given image.\n\nOpenAI's CLIP model aims to learn generic visual concept with natural language supervision. This is because sandard computer vision model only work well on specific task, and require significant effort to adapt to a new task, hence have weak generalization capabilities. CLIP bridges the gap via learning directly from raw text about images at a web scale level.\nCLIP does not directly optimize for the performance of a benchmark task (e.g. CIFAR), so as to keep its \"zero-shot\" capabilities for generalization. More interestingly, CLIP shows that scaling a simple pre-training task - which is to learn \"which text matches with which image\", is sufficient to achieve competitive zero-shot performance on many image classification datasets. \n\n![clipa](https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-a.svg)\n\n![clipb](https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-b.svg)\n\nCLIP trains a text encoder (Bag-of-Words or Text Transformer) and an image encoder (ResNet or Image Transformer) which learns feature representations of a given pair of text and image. The scaled cosine similarity matrix of the image and text feature is computed, and the diagonal values are minimized to force the image feature match its corresponding text feature.\n\n___","metadata":{}},{"cell_type":"markdown","source":"# VQGAN+CLIP:\n\nCLIP guides VQGAN towards an image that is the best match to a given text. CLIP is the “Perceptor” and VQGAN is the “Generator”. VQGAN like all GANs VQGAN takes in a noise vector, and outputs a (realistic) image. CLIP on the other hand takes in an image and text, and outputs the image features and text features respectively. The similarity between image and text can be represented by the cosine similarity of the learnt feature vectors.\n\nBy leveraging CLIPs capacities as a “steering wheel”, we can use CLIP to guide a search through VQGAN’s latent space to find images that match a text prompt very well according to CLIP.\n\nsource - [VQGAN+CLIP — How does it work?](https://alexasteinbruck.medium.com/vqgan-clip-how-does-it-work-210a5dca5e52)\n\n![VQGAN+CLIP — How does it work?](https://ljvmiranda921.github.io/assets/png/vqgan/clip_vqgan_with_image.png)\n\nImage source - [The Illustrated VQGAN](https://ljvmiranda921.github.io/assets/png/vqgan/clip_vqgan_with_image.png)\n___","metadata":{}},{"cell_type":"code","source":"##Install the dependencies \n!pip install --user torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0\n!git clone https://github.com/openai/CLIP\n!pip install taming-transformers\n!git clone https://github.com/CompVis/taming-transformers.git\n!pip install ftfy regex tqdm omegaconf pytorch-lightning\n!pip install kornia\n!pip install imageio-ffmpeg\n!pip install einops\n!mkdir steps","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:10:00.387438Z","iopub.execute_input":"2022-10-03T00:10:00.388185Z","iopub.status.idle":"2022-10-03T00:13:01.245769Z","shell.execute_reply.started":"2022-10-03T00:10:00.388104Z","shell.execute_reply":"2022-10-03T00:13:01.243960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import the Libararies ~","metadata":{}},{"cell_type":"code","source":"!pip install setuptools==59.5.0\n\nimport os\nimport torch\ntorch.hub.download_url_to_file('https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1', \n                               'vqgan_imagenet_f16_16384.yaml')\ntorch.hub.download_url_to_file('https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1', \n                               'vqgan_imagenet_f16_16384.ckpt')\nimport argparse\nimport math\nfrom pathlib import Path\nimport sys\nsys.path.insert(1, './taming-transformers')\n\n# from IPython import display\nfrom base64 import b64encode\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom taming.models import cond_transformer, vqgan\nimport taming.modules \nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nfrom tqdm.notebook import tqdm\nfrom CLIP import clip\nimport kornia.augmentation as K\nimport numpy as np\nimport imageio\nfrom PIL import ImageFile, Image\nfrom urllib.request import urlopen\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom pynvml.smi import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetUtilizationRates\nnvmlInit()\nhandle = nvmlDeviceGetHandleByIndex(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:15:29.270800Z","iopub.execute_input":"2022-10-03T00:15:29.271261Z","iopub.status.idle":"2022-10-03T00:17:02.279131Z","shell.execute_reply.started":"2022-10-03T00:15:29.271226Z","shell.execute_reply":"2022-10-03T00:17:02.274053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**IF you run with an error, restart the kernel, this is due to installing new packages requires kernel restart**","metadata":{}},{"cell_type":"code","source":"#Download sample photos\ntorch.hub.download_url_to_file('https://images.pexels.com/photos/3617500/pexels-photo-3617500.jpeg', \n                               'aurora.jpeg')\ntorch.hub.download_url_to_file('https://images.pexels.com/photos/1660990/pexels-photo-1660990.jpeg', \n                               'iceland_road.jpeg')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:26.826745Z","iopub.execute_input":"2022-10-03T00:17:26.827485Z","iopub.status.idle":"2022-10-03T00:17:28.470708Z","shell.execute_reply.started":"2022-10-03T00:17:26.827455Z","shell.execute_reply":"2022-10-03T00:17:28.468732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Build and Define the Functions**","metadata":{}},{"cell_type":"code","source":"def sinc(x):\n    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n\ndef lanczos(x, a):\n    cond = torch.logical_and(-a < x, x < a)\n    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n    return out / out.sum()\n\ndef ramp(ratio, width):\n    n = math.ceil(width / ratio + 1)\n    out = torch.empty([n])\n    cur = 0\n    for i in range(out.shape[0]):\n        out[i] = cur\n        cur += ratio\n    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n\ndef resample(input, size, align_corners=True):\n    n, c, h, w = input.shape\n    dh, dw = size\n    input = input.view([n * c, 1, h, w])\n    if dh < h:\n        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n        pad_h = (kernel_h.shape[0] - 1) // 2\n        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n        input = F.conv2d(input, kernel_h[None, None, :, None])\n    \n    if dw < w:\n        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n        pad_w = (kernel_w.shape[0] - 1) // 2\n        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n        input = F.conv2d(input, kernel_w[None, None, None, :])\n    input = input.view([n, c, h, w])\n    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:31.818621Z","iopub.execute_input":"2022-10-03T00:17:31.819007Z","iopub.status.idle":"2022-10-03T00:17:31.836283Z","shell.execute_reply.started":"2022-10-03T00:17:31.818979Z","shell.execute_reply":"2022-10-03T00:17:31.834898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReplaceGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x_forward, x_backward):\n        ctx.shape = x_backward.shape\n        return x_forward\n    @staticmethod\n    def backward(ctx, grad_in):\n        return None, grad_in.sum_to_size(ctx.shape)\nreplace_grad = ReplaceGrad.apply\n\nclass ClampWithGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, min, max):\n        ctx.min = min\n        ctx.max = max\n        ctx.save_for_backward(input)\n        return input.clamp(min, max)\n    @staticmethod\n    def backward(ctx, grad_in):\n        input, = ctx.saved_tensors\n        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\nclamp_with_grad = ClampWithGrad.apply","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:35.502180Z","iopub.execute_input":"2022-10-03T00:17:35.502709Z","iopub.status.idle":"2022-10-03T00:17:35.515610Z","shell.execute_reply.started":"2022-10-03T00:17:35.502668Z","shell.execute_reply":"2022-10-03T00:17:35.513893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vector_quantize(x, codebook):\n    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n    indices = d.argmin(-1)\n    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n    return replace_grad(x_q, x)\n\nclass Prompt(nn.Module):\n    def __init__(self, embed, weight=1., stop=float('-inf')):\n        super().__init__()\n        self.register_buffer('embed', embed)\n        self.register_buffer('weight', torch.as_tensor(weight))\n        self.register_buffer('stop', torch.as_tensor(stop))\n    def forward(self, input):\n        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n        dists = dists * self.weight.sign()\n        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n\ndef parse_prompt(prompt):\n    vals = prompt.rsplit(':', 2)\n    vals = vals + ['', '1', '-inf'][len(vals):]\n    return vals[0], float(vals[1]), float(vals[2])","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:38.465363Z","iopub.execute_input":"2022-10-03T00:17:38.465847Z","iopub.status.idle":"2022-10-03T00:17:38.480693Z","shell.execute_reply.started":"2022-10-03T00:17:38.465779Z","shell.execute_reply":"2022-10-03T00:17:38.479179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cutn, cut_pow=1):\n        super().__init__()\n        self.cut_size = cut_size\n        self.cutn = cutn\n        self.cut_pow = cut_pow\n        self.augs = nn.Sequential(\n            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n            K.RandomPerspective(0.7,p=0.7),\n            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n            K.RandomErasing((.1, .4), (.3, 1/.3), same_on_batch=True, p=0.7),\n        )\n        self.noise_fac = 0.1\n        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n\n    def forward(self, input):\n        slideY, slideX = input.shape[2:4]\n        max_size = min(slideX, slideY)\n        min_size = min(slideX, slideY, self.cut_size)\n        cutouts = []\n\n        for _ in range(self.cutn):\n            cutout = (self.av_pool(input) + self.max_pool(input))/2\n            cutouts.append(cutout)\n\n        batch = self.augs(torch.cat(cutouts, dim=0))\n        if self.noise_fac:\n            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n            batch = batch + facs * torch.randn_like(batch)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:41.978719Z","iopub.execute_input":"2022-10-03T00:17:41.979205Z","iopub.status.idle":"2022-10-03T00:17:41.993212Z","shell.execute_reply.started":"2022-10-03T00:17:41.979174Z","shell.execute_reply":"2022-10-03T00:17:41.991073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_vqgan_model(config_path, checkpoint_path):\n    config = OmegaConf.load(config_path)\n    if config.model.target == 'taming.models.vqgan.VQModel':\n        model = vqgan.VQModel(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n\n    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n        model = vqgan.GumbelVQ(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n\n    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n        parent_model.eval().requires_grad_(False)\n        parent_model.init_from_ckpt(checkpoint_path)\n        model = parent_model.first_stage_model\n    else:\n        raise ValueError(f'unknown model type: {config.model.target}')\n    del model.loss\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:44.876797Z","iopub.execute_input":"2022-10-03T00:17:44.877201Z","iopub.status.idle":"2022-10-03T00:17:44.886945Z","shell.execute_reply.started":"2022-10-03T00:17:44.877172Z","shell.execute_reply":"2022-10-03T00:17:44.885501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_image(image, out_size):\n    ratio = image.size[0] / image.size[1]\n    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n    return image.resize(size, Image.LANCZOS)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:48.091166Z","iopub.execute_input":"2022-10-03T00:17:48.091643Z","iopub.status.idle":"2022-10-03T00:17:48.100176Z","shell.execute_reply.started":"2022-10-03T00:17:48.091580Z","shell.execute_reply":"2022-10-03T00:17:48.098493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Download the models","metadata":{}},{"cell_type":"code","source":"model_name = \"vqgan_imagenet_f16_16384\" \nimages_interval =  50\nwidth =  512\nheight = 512\ninit_image = \"\"\nseed = 42\nBASE_PATH = '../input/flickr-image-dataset/flickr30k_images/flickr30k_images/'\nargs = argparse.Namespace(\n    noise_prompt_seeds=[],\n    noise_prompt_weights=[],\n    size=[width, height],\n    init_image=init_image,\n    init_weight=0.,\n    clip_model='ViT-B/32',\n    vqgan_config=f'{model_name}.yaml',\n    vqgan_checkpoint=f'{model_name}.ckpt',\n    step_size=0.13,\n    cutn=32,\n    cut_pow=1.,\n    display_freq=images_interval,\n    seed=seed,\n)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nmodel = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\nperceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:17:50.323570Z","iopub.execute_input":"2022-10-03T00:17:50.324005Z","iopub.status.idle":"2022-10-03T00:18:35.145267Z","shell.execute_reply.started":"2022-10-03T00:17:50.323975Z","shell.execute_reply":"2022-10-03T00:18:35.144028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# # Main Function Defination","metadata":{}},{"cell_type":"code","source":"def inference(text, \n              seed, \n              step_size,\n              max_iterations,\n              width, \n              height,\n              init_image, \n              init_weight, \n              target_images, \n              cutn, \n              cut_pow,\n              video_file\n             ):\n    all_frames = []\n    size=[width, height]\n    texts = text\n    init_weight=init_weight\n\n    if init_image:\n        init_image = init_image\n    else:\n        init_image = \"\"\n    if target_images:\n        target_images = target_images\n    else:\n        target_images = \"\"\n    max_iterations = max_iterations\n    model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n                 \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n                 'vqgan_openimages_f16_8192':'OpenImages 8912',\n                 \"wikiart_1024\":\"WikiArt 1024\", \n                 \"wikiart_16384\":\"WikiArt 16384\", \n                 \"coco\":\"COCO-Stuff\",\n                 \"faceshq\":\"FacesHQ\",\n                 \"sflckr\":\"S-FLCKR\"}\n    name_model = model_names[model_name]\n    if target_images == \"None\" or not target_images:\n        target_images = []\n    else:\n        target_images = target_images.split(\"|\")\n        target_images = [image.strip() for image in target_images]\n\n    texts = [phrase.strip() for phrase in texts.split(\"|\")]\n    if texts == ['']:\n        texts = []\n    if texts:\n        print('Using texts:', texts)\n    if target_images:\n        print('Using image prompts:', target_images)\n    if seed is None or seed == -1:\n        seed = torch.seed()\n    else:\n        seed = seed\n    torch.manual_seed(seed)\n    print('Using seed:', seed)\n\n    cut_size = perceptor.visual.input_resolution\n    f = 2**(model.decoder.num_resolutions - 1)\n    make_cutouts = MakeCutouts(cut_size, cutn, cut_pow=cut_pow)\n    toksX, toksY = size[0] // f, size[1] // f\n    sideX, sideY = toksX * f, toksY * f\n\n    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n        e_dim = 256\n        n_toks = model.quantize.n_embed\n        z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n        z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n    else:\n        e_dim = model.quantize.e_dim\n        n_toks = model.quantize.n_e\n        z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n        z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n\n    if init_image:\n        if 'http' in init_image:\n            img = Image.open(urlopen(init_image))\n        else:\n            img = Image.open(init_image)\n        pil_image = img.convert('RGB')\n        pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n        pil_tensor = TF.to_tensor(pil_image)\n        z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n    else:\n        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n        # z = one_hot @ model.quantize.embedding.weight\n        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n            z = one_hot @ model.quantize.embed.weight\n        else:\n            z = one_hot @ model.quantize.embedding.weight\n        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n        z = torch.rand_like(z)*2\n    z_orig = z.clone()\n    z.requires_grad_(True)\n    opt = optim.Adam([z], lr=step_size)\n    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                                    std=[0.26862954, 0.26130258, 0.27577711])\n    pMs = []\n    for prompt in texts:\n        txt, weight, stop = parse_prompt(prompt)\n        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n        pMs.append(Prompt(embed, weight, stop).to(device))\n    for prompt in target_images:\n        path, weight, stop = parse_prompt(prompt)\n        img = Image.open(path)\n        pil_image = img.convert('RGB')\n        img = resize_image(pil_image, (sideX, sideY))\n        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n        embed = perceptor.encode_image(normalize(batch)).float()\n        pMs.append(Prompt(embed, weight, stop).to(device))\n    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n        gen = torch.Generator().manual_seed(seed)\n        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n        pMs.append(Prompt(embed, weight).to(device))\n\n    def synth(z):\n        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n        else:\n            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n        return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n    @torch.no_grad()\n    def checkin(i, losses):\n        losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n        tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n        out = synth(z)\n        # TF.to_pil_image(out[0].cpu()).save('progress.png')\n        # display.display(display.Image('progress.png'))\n        res = nvmlDeviceGetUtilizationRates(handle)\n        print(f'gpu: {res.gpu}%, gpu-mem: {res.memory}%')\n    def ascend_txt():\n        # global i\n        out = synth(z)\n        iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n        \n        result = []\n        if init_weight:\n            result.append(F.mse_loss(z, z_orig) * init_weight / 2)\n            #result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1/torch.tensor(i*2 + 1))*init_weight) / 2)\n        for prompt in pMs:\n            result.append(prompt(iii))\n        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n        img = np.transpose(img, (1, 2, 0))\n        # imageio.imwrite('./steps/' + str(i) + '.png', np.array(img))\n        img = Image.fromarray(img).convert('RGB')\n        all_frames.append(img)\n        return result, np.array(img)\n    def train(i):\n        opt.zero_grad()\n        lossAll, image = ascend_txt()\n        if i % args.display_freq == 0:\n            checkin(i, lossAll)\n        \n        loss = sum(lossAll)\n        loss.backward()\n        opt.step()\n        with torch.no_grad():\n            z.copy_(z.maximum(z_min).minimum(z_max))\n        return image\n    i = 0\n    try:\n        with tqdm() as pbar:\n            while True:\n                image = train(i)\n                if i == max_iterations:\n                    break\n                i += 1\n                pbar.update()\n    except KeyboardInterrupt:\n        pass\n    writer = imageio.get_writer(video_file + '.mp4', fps=20)\n    for im in all_frames:\n        writer.append_data(np.array(im))\n    writer.close()\n    # all_frames[0].save('out.gif',\n              # save_all=True, append_images=all_frames[1:], optimize=False, duration=80, loop=0)\n    return image","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:19:27.110991Z","iopub.execute_input":"2022-10-03T00:19:27.111406Z","iopub.status.idle":"2022-10-03T00:19:27.149259Z","shell.execute_reply.started":"2022-10-03T00:19:27.111375Z","shell.execute_reply":"2022-10-03T00:19:27.147589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_image( infilename ) :\n    img = Image.open( infilename )\n    img.load()\n    data = np.asarray( img, dtype=\"int32\" )\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:19:32.282789Z","iopub.execute_input":"2022-10-03T00:19:32.283514Z","iopub.status.idle":"2022-10-03T00:19:32.294667Z","shell.execute_reply.started":"2022-10-03T00:19:32.283464Z","shell.execute_reply":"2022-10-03T00:19:32.293005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_result(img) :\n    plt.figure(figsize=(9,9))\n    plt.imshow(img)\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:19:34.136268Z","iopub.execute_input":"2022-10-03T00:19:34.136655Z","iopub.status.idle":"2022-10-03T00:19:34.143312Z","shell.execute_reply.started":"2022-10-03T00:19:34.136626Z","shell.execute_reply":"2022-10-03T00:19:34.141680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 1","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'underwater city ', \n    seed = 2,\n    step_size = 0.12,\n    max_iterations = 300,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.004,\n    target_images = '', \n    cutn = 64,\n    cut_pow = 0.3,\n    video_file = \"test1\"\n)\ndisplay_result(img)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:19:37.743009Z","iopub.execute_input":"2022-10-03T00:19:37.743404Z","iopub.status.idle":"2022-10-03T00:23:23.057478Z","shell.execute_reply.started":"2022-10-03T00:19:37.743376Z","shell.execute_reply":"2022-10-03T00:23:23.056038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Download the output in video format\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('test1.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:23:58.461074Z","iopub.execute_input":"2022-10-03T00:23:58.461525Z","iopub.status.idle":"2022-10-03T00:23:58.560634Z","shell.execute_reply.started":"2022-10-03T00:23:58.461491Z","shell.execute_reply":"2022-10-03T00:23:58.558876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 2","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'winter in train', \n    seed = 191,\n    step_size = 0.13,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"winter_in_train\"\n)\ndisplay_result(img)\n\nmp4 = open('winter_in_train.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:28:11.323308Z","iopub.execute_input":"2022-10-03T00:28:11.323749Z","iopub.status.idle":"2022-10-03T00:37:02.120021Z","shell.execute_reply.started":"2022-10-03T00:28:11.323720Z","shell.execute_reply":"2022-10-03T00:37:02.118110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 3 ","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'mutation tree and flower',\n    seed =  79472135470,\n    step_size = 0.12,\n    max_iterations = 300,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.024,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"mutation\"\n)\ndisplay_result(img)\n\nmp4 = open('mutation.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:48:20.376735Z","iopub.execute_input":"2022-10-03T00:48:20.377234Z","iopub.status.idle":"2022-10-03T00:51:20.070026Z","shell.execute_reply.started":"2022-10-03T00:48:20.377205Z","shell.execute_reply":"2022-10-03T00:51:20.068004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 4","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'Angels of the Universe',\n    seed = 1011, \n    step_size = 0.12,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"angels_of_the_universe\"\n)\ndisplay_result(img)\n\nmp4 = open('angels_of_the_universe.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:52:01.992487Z","iopub.execute_input":"2022-10-03T00:52:01.992973Z","iopub.status.idle":"2022-10-03T00:58:59.725672Z","shell.execute_reply.started":"2022-10-03T00:52:01.992941Z","shell.execute_reply":"2022-10-03T00:58:59.724024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 5 ~ with init_image","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'Fireflies in the Garden', \n    seed = 201,\n    step_size = 0.12,\n    max_iterations = 400,\n    width = 512,\n    height = 512,\n    init_image = 'https://i1.sndcdn.com/artworks-3TjRVLDyziCxsPFm-eaTwZw-t500x500.jpg',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"fireflies\"\n)\ndisplay_result(img)\n\nmp4 = open('fireflies.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T00:37:31.416032Z","iopub.execute_input":"2022-10-03T00:37:31.416437Z","iopub.status.idle":"2022-10-03T00:42:31.797351Z","shell.execute_reply.started":"2022-10-03T00:37:31.416403Z","shell.execute_reply":"2022-10-03T00:42:31.795044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Test Run 6 ~ with target_images","metadata":{}},{"cell_type":"code","source":"img = inference(\n    text = 'Fireflies in the Garden', \n    seed = 201,\n    step_size = 0.12,\n    max_iterations = 400,\n    width = 512,\n    height = 512,\n    init_image = 'https://i1.sndcdn.com/artworks-3TjRVLDyziCxsPFm-eaTwZw-t500x500.jpg',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"fireflies\"\n)\ndisplay_result(img)\n\nmp4 = open('fireflies.mp4','rb').read()\ndata_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video/mp4\">\n</video>\n\"\"\" % data_url)","metadata":{"execution":{"iopub.status.busy":"2022-10-03T01:00:49.789554Z","iopub.execute_input":"2022-10-03T01:00:49.790011Z","iopub.status.idle":"2022-10-03T01:05:50.433208Z","shell.execute_reply.started":"2022-10-03T01:00:49.789965Z","shell.execute_reply":"2022-10-03T01:05:50.431961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Try out with different combinations\n![](http://)","metadata":{}},{"cell_type":"markdown","source":"# References\n\n* [VQGAN_CLIP( hHugging Face )](https://huggingface.co/spaces/akhaliq/VQGAN_CLIP)\n* [The Illustrated VQGAN](https://ljvmiranda921.github.io/notebook/2021/08/08/clip-vqgan/)\n* [VQGAN+CLIP — How does it work?](https://alexasteinbruck.medium.com/vqgan-clip-how-does-it-work-210a5dca5e52)\n* [Image Generation Based on Abstract Concepts Using CLIP + BigGAN](https://wandb.ai/gudgud96/big-sleep-test/reports/Image-Generation-Based-on-Abstract-Concepts-Using-CLIP-BigGAN--Vmlldzo1MjA2MTE)\n","metadata":{}}]}